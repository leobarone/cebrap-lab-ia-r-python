{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d60bf7c6",
   "metadata": {},
   "source": [
    "# Tutorial — Proposições da Câmara: da API ao corpus de PDFs (com **Python**)\n",
    "\n",
    "Neste tutorial, voltado às pessoas já habituadas à linguagem nos primeiros encontros, vamos trabalhar com dados de proposições de Projeto de Lei na Câmara dos Deputados, criando um corpus a partir dos dados obtidos na API da Câmara, passando pelo download dos arquivos até a organização de um corpus.\n",
    "\n",
    "> **Resumo do que vamos fazer**\n",
    ">\n",
    "> 1. Consultar a API da Câmara para listar os **PLs de 2025**;\n",
    "> 2. Buscar os metadados de cada proposição e obter os **id** e endereços dos arquivos originais das proposições de inteiro teor;\n",
    "> 3. **Baixar** os PDFs de inteiro teor (com fallback via `/proposicoes/{id}/arquivos`);\n",
    "> 4. Construir um **corpus** a partir dos PDFs para análises textuais.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccf6643",
   "metadata": {},
   "source": [
    "## 0) Preparação do ambiente\n",
    "\n",
    "Se estiver rodando localmente, instale as dependências (apenas na primeira vez):\n",
    "\n",
    "```bash\n",
    "pip install requests pandas pdfminer.six PyPDF2 nltk unidecode\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e99f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports principais\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, Any, List\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# NLP util\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from unidecode import unidecode\n",
    "\n",
    "# leitura de PDFs (opcional; se não tiver instalado, o notebook segue sem quebrar)\n",
    "try:\n",
    "    from pdfminer.high_level import extract_text  # para extrair o texto\n",
    "except Exception:\n",
    "    extract_text = None\n",
    "\n",
    "try:\n",
    "    from PyPDF2 import PdfReader  # para contagem de páginas\n",
    "except Exception:\n",
    "    PdfReader = None\n",
    "\n",
    "# downloads de recursos do NLTK (somente se ainda não tiver)\n",
    "try:\n",
    "    _ = stopwords.words('portuguese')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    _ = stopwords.words('portuguese')\n",
    "\n",
    "pd.__version__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759bb783",
   "metadata": {},
   "source": [
    "---\n",
    "## 1) Sessão HTTP robusta e listagem de proposições\n",
    "\n",
    "Vamos usar uma `requests.Session` com **retry** e `User-Agent` explícito. Em seguida, listamos os PLs de 2025.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e010ebce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "\n",
    "def make_session() -> requests.Session:\n",
    "    s = requests.Session()\n",
    "    s.headers.update({\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"User-Agent\": \"cebrap-lab-ia/1.0 (+https://github.com/cebrap-lab)\"\n",
    "    })\n",
    "    retries = Retry(\n",
    "        total=5, backoff_factor=0.5,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"GET\"]\n",
    "    )\n",
    "    s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "    s.mount(\"http://\", HTTPAdapter(max_retries=retries))\n",
    "    return s\n",
    "\n",
    "SESSION = make_session()\n",
    "BASE = \"https://dadosabertos.camara.leg.br/api/v2/proposicoes\"\n",
    "\n",
    "def listar_proposicoes(sigla_tipo: str = \"PL\", ano: int = 2025, itens: int = 100) -> pd.DataFrame:\n",
    "    params = {\"siglaTipo\": sigla_tipo, \"ano\": ano, \"ordem\": \"ASC\", \"ordenarPor\": \"id\", \"itens\": itens}\n",
    "    r = SESSION.get(BASE, params=params, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    return pd.DataFrame(r.json().get(\"dados\", []))\n",
    "\n",
    "df_proposicoes_simples = listar_proposicoes(\"PL\", 2025, itens=100)\n",
    "df_proposicoes_simples.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfac399",
   "metadata": {},
   "source": [
    "> **Nota sobre paginação:** para mais de 100 registros, itere com o parâmetro `pagina` e concatene os resultados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f25b36e",
   "metadata": {},
   "source": [
    "---\n",
    "## 2) Função para buscar o **detalhe** de uma proposição\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb63db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obter_proposicao_detalhe(pid: int) -> Optional[Dict[str, Any]]:\n",
    "    url = f\"{BASE}/{pid}\"\n",
    "    r = SESSION.get(url, timeout=60)\n",
    "    if not r.ok:\n",
    "        return None\n",
    "    d = r.json().get(\"dados\", {}) or {}\n",
    "    st = d.get(\"statusProposicao\") or {}\n",
    "    return {\n",
    "        \"id\": d.get(\"id\"),\n",
    "        \"siglaTipo\": d.get(\"siglaTipo\"),\n",
    "        \"numero\": d.get(\"numero\"),\n",
    "        \"ano\": d.get(\"ano\"),\n",
    "        \"ementa\": d.get(\"ementa\"),\n",
    "        \"dataApresentacao\": d.get(\"dataApresentacao\"),\n",
    "        \"urlInteiroTeor\": d.get(\"urlInteiroTeor\"),\n",
    "        \"status_apreciacao\": st.get(\"apreciacao\"),\n",
    "    }\n",
    "\n",
    "detalhes = []\n",
    "for pid in df_proposicoes_simples[\"id\"].tolist():\n",
    "    time.sleep(0.1)  # cortesia com a API\n",
    "    d = obter_proposicao_detalhe(int(pid))\n",
    "    if d:\n",
    "        detalhes.append(d)\n",
    "\n",
    "df_proposicoes = pd.DataFrame(detalhes)\n",
    "df_proposicoes.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0860c9",
   "metadata": {},
   "source": [
    "---\n",
    "## 3) Resolvendo o **URL do PDF** com fallback em `/proposicoes/{id}/arquivos`\n",
    "\n",
    "Muitas vezes `urlInteiroTeor` está vazio no detalhe. Por isso, buscamos em `/arquivos` e escolhemos um link que seja PDF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e496dcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def listar_arquivos(pid: int) -> List[Dict[str, Any]]:\n",
    "    url = f\"{BASE}/{pid}/arquivos\"\n",
    "    r = SESSION.get(url, timeout=60)\n",
    "    if not r.ok:\n",
    "        return []\n",
    "    return r.json().get(\"dados\", []) or []\n",
    "\n",
    "def escolher_url_pdf(row: Dict[str, Any]) -> Optional[str]:\n",
    "    # 1) tenta o campo direto do detalhe\n",
    "    url = row.get(\"urlInteiroTeor\")\n",
    "    if url and isinstance(url, str) and url.strip():\n",
    "        return url\n",
    "\n",
    "    # 2) cai para /arquivos e procura PDF\n",
    "    arquivos = listar_arquivos(int(row[\"id\"]))\n",
    "    campos = (\"urlDownload\", \"urlInteiroTeor\", \"url\")\n",
    "    for arq in arquivos:\n",
    "        eh_pdf = (\n",
    "            (arq.get(\"formato\") or \"\").lower() == \"pdf\" or\n",
    "            str(arq.get(\"nome\") or \"\").lower().endswith(\".pdf\") or\n",
    "            \"pdf\" in str(arq.get(\"titulo\") or \"\").lower()\n",
    "        )\n",
    "        if not eh_pdf:\n",
    "            continue\n",
    "        for c in campos:\n",
    "            link = arq.get(c)\n",
    "            if link and isinstance(link, str) and link.strip():\n",
    "                return link\n",
    "\n",
    "    # 3) último recurso: primeira URL disponível\n",
    "    for arq in arquivos:\n",
    "        for c in campos:\n",
    "            link = arq.get(c)\n",
    "            if link and isinstance(link, str) and link.strip():\n",
    "                return link\n",
    "    return None\n",
    "\n",
    "df_proposicoes = df_proposicoes.copy()\n",
    "df_proposicoes[\"url_pdf\"] = df_proposicoes.apply(escolher_url_pdf, axis=1)\n",
    "df_proposicoes[[\"id\",\"urlInteiroTeor\",\"url_pdf\"]].head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805d224d",
   "metadata": {},
   "source": [
    "---\n",
    "## 4) **Baixar** os PDFs (robusto)\n",
    "\n",
    "Usamos a mesma `Session` com retries e checamos `Content-Type` quando disponível.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b26540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baixar_pdf(pid: int, url: str, outdir: str = \"inteiro_teor\") -> bool:\n",
    "    Path(outdir).mkdir(parents=True, exist_ok=True)\n",
    "    destino = Path(outdir) / f\"{pid}.pdf\"\n",
    "    try:\n",
    "        with SESSION.get(url, stream=True, timeout=120, headers={\"Accept\": \"*/*\"}) as r:\n",
    "            r.raise_for_status()\n",
    "            # Se o servidor não enviar content-type de PDF, seguimos mesmo assim\n",
    "            with open(destino, \"wb\") as f:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "        # tamanho mínimo pra considerar válido\n",
    "        return destino.exists() and destino.stat().st_size > 1024\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "outdir = \"inteiro_teor\"\n",
    "baixados = 0\n",
    "for _, row in df_proposicoes.iterrows():\n",
    "    if not row[\"url_pdf\"]:\n",
    "        continue\n",
    "    time.sleep(0.15)  # cortesia\n",
    "    if baixar_pdf(int(row[\"id\"]), row[\"url_pdf\"], outdir=outdir):\n",
    "        baixados += 1\n",
    "\n",
    "baixados, sorted([p.name for p in Path(outdir).glob(\"*.pdf\")])[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183f0f82",
   "metadata": {},
   "source": [
    "---\n",
    "## 5) Lendo os PDFs e montando um **corpus**\n",
    "\n",
    "Se `pdfminer.six` e `PyPDF2` estiverem instalados, extraímos texto e número de páginas. Caso contrário, o notebook segue sem quebrar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6a8be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ler_pdf(arquivo_pdf: str, pasta: str = \"inteiro_teor\"):\n",
    "    caminho = Path(pasta) / arquivo_pdf\n",
    "    # texto\n",
    "    texto = \"\"\n",
    "    if extract_text is not None:\n",
    "        try:\n",
    "            texto = extract_text(str(caminho)) or \"\"\n",
    "        except Exception:\n",
    "            texto = \"\"\n",
    "    # páginas\n",
    "    n_paginas = None\n",
    "    if PdfReader is not None:\n",
    "        try:\n",
    "            reader = PdfReader(str(caminho))\n",
    "            n_paginas = len(reader.pages)\n",
    "        except Exception:\n",
    "            n_paginas = None\n",
    "    _id = arquivo_pdf.replace(\".pdf\", \"\")\n",
    "    return {\"id\": _id, \"n_paginas\": n_paginas, \"text\": texto, \"n_caracteres\": len(texto)}\n",
    "\n",
    "arquivos = sorted([p.name for p in Path(\"inteiro_teor\").glob(\"*.pdf\")])\n",
    "corpus_docs = pd.DataFrame([ler_pdf(a) for a in arquivos])\n",
    "corpus_docs.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44922749",
   "metadata": {},
   "source": [
    "---\n",
    "## 6) **(Opcional)**: análises de texto (tokenização simples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8382684",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_pt = set(stopwords.words('portuguese'))\n",
    "\n",
    "def preprocess_text(s: str) -> str:\n",
    "    s = (s or \"\").lower()\n",
    "    s = unidecode(s)\n",
    "    return s\n",
    "\n",
    "tokens = []\n",
    "for _, row in corpus_docs.iterrows():\n",
    "    txt = preprocess_text(row.get(\"text\", \"\"))\n",
    "    words = [w for w in txt.split() if w.isalpha() and w not in stop_pt]\n",
    "    tokens.extend(words)\n",
    "\n",
    "top_palavras = (\n",
    "    pd.Series(tokens, name=\"word\")\n",
    "    .value_counts()\n",
    "    .head(30)\n",
    "    .rename_axis(\"word\")\n",
    "    .reset_index(name=\"n\")\n",
    ")\n",
    "top_palavras\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
