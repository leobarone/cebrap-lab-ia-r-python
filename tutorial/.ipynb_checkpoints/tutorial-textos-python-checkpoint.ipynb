{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8a01ecf",
   "metadata": {},
   "source": [
    "\n",
    "# Preparação e Análise de Texto em **Python** com *pandas* e *NLTK*\n",
    "\n",
    "Neste tutorial faremos uma rápida introdução em **Python** sobre a **preparação** de dados textuais para **análise**. Nosso objetivo é saber transformar e identificar padrões em textos para que possamos organizar e preparar uma coleção de documentos para análise após a coleta via raspagem de dados e/ou após a extração a partir de documentos de imagem/pdf.\n",
    "\n",
    "Com as bibliotecas de strings do Python (via `re` e utilitários como `unidecode`) aprenderemos a fazer manipulações simples nos conjuntos de dados, como buscas com **expressões regulares**, remoção de caracteres, transformação em **maiúsculas/minúsculas**, remoção de **espaços extras** etc.\n",
    "\n",
    "A seguir, utilizaremos `pandas` (data frames) e funções de tokenização do `nltk` para organizar um **corpus** em formato “*tidy*” (uma linha por token) e tornar a manipulação das informações relativamente simples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727664c6",
   "metadata": {},
   "source": [
    "\n",
    "> Referências úteis (em espírito semelhante às do tutorial original):  \n",
    "> - Documentação `re` (regex): https://docs.python.org/pt-br/3/library/re.html  \n",
    "> - NLTK Book: https://www.nltk.org/book/  \n",
    "> - pandas: https://pandas.pydata.org/docs/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd05e20e",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Preparação para o tutorial: pacotes e objeto *corpus*\n",
    "Vamos utilizar os seguintes pacotes:\n",
    "\n",
    "* **pandas**: manipulação de dados em data frames.\n",
    "* **regex (`re`)**: buscas e transformações com expressões regulares.\n",
    "* **unidecode**: remoção de acentos.\n",
    "* **nltk**: tokenização e stopwords (português e inglês).\n",
    "* **matplotlib**: gráficos (barras de frequências).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b482c26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Se necessário, instale os pacotes (descomente se estiver em um ambiente local):\n",
    "# !pip install pandas unidecode nltk matplotlib\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Download de recursos do NLTK (somente na primeira vez)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6b526d",
   "metadata": {},
   "source": [
    "\n",
    "**O que está acontecendo acima?**  \n",
    "Carregamos as bibliotecas e, no `NLTK`, baixamos recursos necessários (`stopwords` e `punkt`) — semelhante a preparar o ambiente no R com `library(...)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12db2df",
   "metadata": {},
   "source": [
    "\n",
    "Vamos agora carregar os textos que analisaremos. Há no curso um [tutorial sobre como criar um corpus a partir dos PDFs de proposições legislativas em Python](https://github.com/leobarone/cebrap-lab-ia-r-python/blob/main/tutorial/tutorial-proposicoes-legislativas-python.ipynb) sobre como criar um corpus a partir de PDFs de proposições legislativas. \n",
    "\n",
    "**Corpus**, no contexto da análise de dados, é o conjunto de documentos textuais. Em Python, vamos representá-lo como um `DataFrame` com, no mínimo, duas colunas: `id` e `texto`. Pode conter também metadados (autoria, data etc.).\n",
    "\n",
    "O *corpus* que vamos utilizar neste tutorial pode ser carregado diretamente de `data/corpus_docs.csv` (mesmo caminho citado no material do curso).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9821a598",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url_corpus = \"https://raw.githubusercontent.com/leobarone/cebrap-lab-ia-r-python/main/tutorial/data/corpus_docs.csv\"\n",
    "corpus_docs = pd.read_csv(url_corpus, sep=\";\", encoding=\"utf-8\")\n",
    "corpus_docs = corpus_docs.rename(columns={\"text\": \"texto\"})\n",
    "corpus_docs.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c51fc45",
   "metadata": {},
   "source": [
    "\n",
    "**Por que isso?**  \n",
    "Lemos o CSV hospedado no GitHub (formato *raw*), ajustando o separador `;` (equivalente ao `read_csv2` do R) e renomeamos a coluna `text` para `texto` para manter consistência com o restante do tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e489bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corpus_docs['texto'].iloc[0][:1000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6524cfe9",
   "metadata": {},
   "source": [
    "\n",
    "**Inspeção rápida**  \n",
    "Mostramos os primeiros 1000 caracteres do primeiro documento — útil para verificar ruídos vindos de OCR/PDF antes da limpeza.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ff5abe",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Limpeza e preparação de textos (equivalentes em Python)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e5df6f",
   "metadata": {},
   "source": [
    "\n",
    "### Remover acentos\n",
    "Aplicamos uma função simples com `unidecode` para converter caracteres acentuados em ASCII, tal como a função `remove_acentos()` em R via `iconv(...)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b33a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_acentos(s: str) -> str:\n",
    "    return unidecode(s) if isinstance(s, str) else s\n",
    "\n",
    "corpus_docs['texto'] = corpus_docs['texto'].apply(remove_acentos)\n",
    "corpus_docs['texto'].iloc[0][:1000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8053041",
   "metadata": {},
   "source": [
    "\n",
    "**Resultado esperado**  \n",
    "Os acentos (e.g., `ação` → `acao`) e cedilhas (`ç` → `c`) são removidos, uniformizando o texto para buscas e contagens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c490983",
   "metadata": {},
   "source": [
    "\n",
    "### Padronizar minúsculas/maiúsculas\n",
    "Transformamos tudo em **minúsculas** (equivalente a `str_to_lower`), reduzindo variações artificiais entre termos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2162e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corpus_docs['texto'] = corpus_docs['texto'].str.lower()\n",
    "corpus_docs['texto'].iloc[0][:1000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9112c274",
   "metadata": {},
   "source": [
    "\n",
    "**Por que minúsculas?**  \n",
    "Evita que `Lei`, `lei` e `LEI` sejam tratados como palavras distintas na análise de frequência e n-gramas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35525832",
   "metadata": {},
   "source": [
    "\n",
    "### Remover pontuação (regex)\n",
    "Usamos `str.replace` com expressão regular para remover tudo que **não** é palavra (`\\w`) nem espaço. É o análogo do `str_remove_all(texto, '[:punct:]')`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb100335",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fallback para ambientes sem suporte a classes unicode: remove tudo que não for \\w ou espaço\n",
    "corpus_docs['texto'] = corpus_docs['texto'].str.replace(r'[^\\w\\s]', ' ', regex=True)\n",
    "corpus_docs['texto'].iloc[0][:1000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bdecdd",
   "metadata": {},
   "source": [
    "\n",
    "**Dica**  \n",
    "Dependendo do corpus, você pode preservar dígitos (`\\d`) ou hífens, ajustando a regex ao seu caso.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed74b5a",
   "metadata": {},
   "source": [
    "\n",
    "### Normalização de espaços (equivalente ao `str_squish`)\n",
    "Compactamos **múltiplos espaços** em um só e removemos espaços nas extremidades (começo/fim).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec267d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corpus_docs['texto'] = (corpus_docs['texto']\n",
    "                        .str.replace(r'\\s+', ' ', regex=True)\n",
    "                        .str.strip())\n",
    "corpus_docs['texto'].iloc[0][:1000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb36a6b",
   "metadata": {},
   "source": [
    "\n",
    "**Quando usar**  \n",
    "Após remoções com regex, sobram lacunas de espaço. Esta etapa consolida o texto para tokenizações mais limpas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544397ef",
   "metadata": {},
   "source": [
    "\n",
    "Vejamos como ficaram os primeiros mil caracteres do primeiro texto depois de modificado:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f230d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "primeiro_texto = corpus_docs['texto'].iloc[0]\n",
    "primeiro_texto[:1000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99abe02e",
   "metadata": {},
   "source": [
    "\n",
    "**Checagem**  \n",
    "Útil para validar se a sequência de limpeza atingiu o efeito esperado antes de seguir para as métricas e tokens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9104a1",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Medidas simples e detecção de padrões\n",
    "**Qual dos textos do nosso *corpus* é o mais longo?** Vamos criar uma variável de **tamanho** (em caracteres):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53dcc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corpus_docs['tamanho'] = corpus_docs['texto'].str.len()\n",
    "corpus_docs.sort_values('tamanho', ascending=False).head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcec5914",
   "metadata": {},
   "source": [
    "\n",
    "**Interpretação**  \n",
    "A coluna `tamanho` nos permite identificar rapidamente documentos extensos (que podem exigir tratamento especial ou amostragem).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6beef5",
   "metadata": {},
   "source": [
    "\n",
    "**Identificar padrões** (“jornada de trabalho”, “meio ambiente”) com operações vetorizadas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8ba05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "padrao_jornada = corpus_docs['texto'].str.contains('jornada de trabalho', regex=False)\n",
    "padrao_meioamb = corpus_docs['texto'].str.contains('meio ambiente', regex=False)\n",
    "\n",
    "indices_jornada = padrao_jornada[padrao_jornada].index.tolist()\n",
    "indices_meioamb = padrao_meioamb[padrao_meioamb].index.tolist()\n",
    "\n",
    "subset_jornada = corpus_docs.loc[padrao_jornada, 'texto']\n",
    "subset_meioamb = corpus_docs.loc[padrao_meioamb, 'texto']\n",
    "\n",
    "padrao_jornada, indices_jornada, subset_jornada.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23987a6",
   "metadata": {},
   "source": [
    "\n",
    "**Equivalências ao R**  \n",
    "`str.contains` ≈ `str_detect`; índices via `.index.tolist()` ≈ `str_which`; subconjuntos obtidos por máscara booleana ≈ `str_subset`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47015115",
   "metadata": {},
   "source": [
    "\n",
    "**Marcar no DataFrame** (em vez de filtrar): criar colunas lógicas e de **contagem** de ocorrências:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020caa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corpus_docs = corpus_docs.assign(\n",
    "    tem_jornada = corpus_docs['texto'].str.contains('jornada de trabalho', regex=False),\n",
    "    tem_meioamb = corpus_docs['texto'].str.contains('meio ambiente', regex=False),\n",
    "    n_jornada   = corpus_docs['texto'].str.count('jornada de trabalho'),\n",
    "    n_meioamb   = corpus_docs['texto'].str.count('meio ambiente')\n",
    ")\n",
    "corpus_docs.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a4c49d",
   "metadata": {},
   "source": [
    "\n",
    "**Uso prático**  \n",
    "Essas colunas permitem filtros analíticos (e.g., documentos com `tem_meioamb == True`) e análises de intensidade (`n_jornada`, `n_meioamb`). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9253a0c7",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Tidy text: tokens, stopwords e frequências\n",
    "\n",
    "Agora vamos organizar o texto em **formato *tidy***: cada linha corresponde a um **token** (palavra, bigram, trigram…). Começaremos por **palavras**.\n",
    "\n",
    "> A ideia central: transformar a coluna `texto` em muitos registros, um por token, preservando a ligação com `id`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecbf4f3",
   "metadata": {},
   "source": [
    "\n",
    "### Tokenização em palavras\n",
    "Usaremos o **padrão do NLTK** com `RegexpTokenizer(r\"\\w+\")`, que ignora pontuação e é robusto para português.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c435037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rtok = RegexpTokenizer(r\"\\w+\")\n",
    "tokens = (\n",
    "    corpus_docs[[\"id\",\"texto\"]]\n",
    "    .assign(word=lambda df: df[\"texto\"].apply(lambda s: rtok.tokenize(s) if isinstance(s, str) else []))\n",
    "    .explode(\"word\")\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "# Remove strings vazias e tokens de 1 caractere\n",
    "tokens = tokens.loc[tokens[\"word\"].str.len() > 1].copy()\n",
    "tokens.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fe338c",
   "metadata": {},
   "source": [
    "\n",
    "**Observação**  \n",
    "A coluna `id` é preservada; `texto` se transforma em `word`. Esse formato *tidy* simplifica contagens e junções com metadados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c23bce",
   "metadata": {},
   "source": [
    "\n",
    "### Stopwords (pt e en) e filtro por tamanho\n",
    "Removeremos palavras muito comuns (pouco informativas) com as listas do NLTK e manteremos tokens com 2+ caracteres.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54485bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stops = set(stopwords.words('portuguese'))\n",
    "tokens = tokens.loc[~tokens['word'].isin(stops)].copy()\n",
    "tokens = tokens.loc[tokens['word'].str.len() > 1].copy()\n",
    "tokens.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7468c99d",
   "metadata": {},
   "source": [
    "\n",
    "**Por que filtrar?**  \n",
    "Isso reduz ruído e foca em termos mais descritivos do conteúdo (nomes, substantivos compostos etc.).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996ae75a",
   "metadata": {},
   "source": [
    "\n",
    "### Frequência de palavras\n",
    "Calcularemos a contagem global de tokens e inspecionaremos os principais termos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727a3251",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "freq = tokens['word'].value_counts().reset_index()\n",
    "freq.columns = ['word','n']\n",
    "freq.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574ce1d2",
   "metadata": {},
   "source": [
    "\n",
    "**Leitura**  \n",
    "A tabela exibe os tokens mais frequentes após limpeza e remoção de stopwords.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21834809",
   "metadata": {},
   "source": [
    "\n",
    "### Gráfico de barras dos termos mais frequentes\n",
    "Visualizaremos os *top-N* termos. Ajuste `top_n` conforme o tamanho do corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ab3064",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "top_n = 20  # ajuste conforme seu corpus\n",
    "top = freq.head(top_n).iloc[::-1]  # invertido para plot vertical agradável\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.barh(top['word'], top['n'])\n",
    "plt.xlabel('Frequência')\n",
    "plt.ylabel('Palavra')\n",
    "plt.title('Termos mais frequentes')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01418d16",
   "metadata": {},
   "source": [
    "\n",
    "**Dica visual**  \n",
    "Barras horizontais facilitam a leitura de rótulos longos; experimente diferentes `top_n` e filtros temáticos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54375a94",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Bigrams e n-grams\n",
    "\n",
    "**Bigrams (pares de palavras)** por documento e contagens globais, agora com `nltk.util.ngrams` usando o mesmo tokenizador e stopwords.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6a293e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def doc_ngrams(texto: str, n: int = 2):\n",
    "    if not isinstance(texto, str):\n",
    "        return []\n",
    "    toks = rtok.tokenize(texto)\n",
    "    toks = [t for t in toks if (t not in stops and len(t) > 1)]\n",
    "    return list(ngrams(toks, n))\n",
    "\n",
    "# Gera bigrams por doc e \"explode\"\n",
    "bigrams_df = (\n",
    "    corpus_docs[[\"id\",\"texto\"]]\n",
    "    .assign(bigrams=lambda df: df[\"texto\"].apply(lambda s: doc_ngrams(s, n=2)))\n",
    "    .explode(\"bigrams\")\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "bigrams_df[[\"word1\",\"word2\"]] = pd.DataFrame(bigrams_df[\"bigrams\"].tolist(), index=bigrams_df.index)\n",
    "\n",
    "bigram_freq = (\n",
    "    bigrams_df\n",
    "    .groupby([\"word1\",\"word2\"])  \n",
    "    .size()\n",
    "    .reset_index(name=\"n\")\n",
    "    .sort_values(\"n\", ascending=False)\n",
    ")\n",
    "bigram_freq.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4190b9a0",
   "metadata": {},
   "source": [
    "\n",
    "**Por que bigrams?**  \n",
    "Capturam **coocorrências** (e.g., “meio ambiente”) que se perdem na análise de palavras isoladas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb184d90",
   "metadata": {},
   "source": [
    "\n",
    "Exemplos de consultas: palavras que antecedem **“trabalho”** e que sucedem **“ambiente”**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccf306b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "antecedem_trabalho = (\n",
    "    bigram_freq[bigram_freq['word2']=='trabalho']\n",
    "    .sort_values('n', ascending=False)\n",
    "    .head(10)\n",
    ")\n",
    "sucedem_ambiente = (\n",
    "    bigram_freq[bigram_freq['word1']=='ambiente']\n",
    "    .sort_values('n', ascending=False)\n",
    "    .head(10)\n",
    ")\n",
    "\n",
    "antecedem_trabalho, sucedem_ambiente\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1aea85",
   "metadata": {},
   "source": [
    "\n",
    "**Leitura rápida**  \n",
    "Essas tabelas mostram os contextos mais comuns imediatamente antes/depois dos termos de interesse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923520db",
   "metadata": {},
   "source": [
    "\n",
    "### Trigrams (3-palavras)\n",
    "Aplicamos a mesma lógica para sequências de três palavras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce02aebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trigrams_df = (\n",
    "    corpus_docs[[\"id\",\"texto\"]]\n",
    "    .assign(trigrams=lambda df: df[\"texto\"].apply(lambda s: doc_ngrams(s, n=3)))\n",
    "    .explode(\"trigrams\")\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "trigrams_df[[\"word1\",\"word2\",\"word3\"]] = pd.DataFrame(trigrams_df[\"trigrams\"].tolist(), index=trigrams_df.index)\n",
    "\n",
    "trigram_freq = (\n",
    "    trigrams_df\n",
    "    .groupby([\"word1\",\"word2\",\"word3\"])  \n",
    "    .size()\n",
    "    .reset_index(name=\"n\")\n",
    "    .sort_values(\"n\", ascending=False)\n",
    ")\n",
    "trigram_freq.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692a38a7",
   "metadata": {},
   "source": [
    "\n",
    "**Quando usar**  \n",
    "Trigrams ajudam a capturar expressões mais estáveis (e.g., “projeto de lei complementar”), úteis para análise temática.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
