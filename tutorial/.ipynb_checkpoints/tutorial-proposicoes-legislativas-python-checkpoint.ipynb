{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b5e4176",
   "metadata": {},
   "source": [
    "# Tutorial — Proposições da Câmara: da API ao corpus de PDFs (com **Python**)\n",
    "\n",
    "Neste tutorial, voltado às pessoas já habituadas à linguagem nos primeiros encontros, vamos trabalhar com dados de proposições de Projeto de Lei na Câmara dos Deputados, criando um corpus a partir dos dados obtidos na API da Câmara, passando pelo download dos arquivos até a organização de um corpus.\n",
    "\n",
    "> **Resumo do que vamos fazer**\n",
    ">\n",
    "> 1. Consultar a API da Câmara para listar os **PLs de 2025**;\n",
    "> 2. Buscar os metadados de cada proposição e obter os **id** e endereços dos arquivos originais das proposições de inteiro teor;\n",
    "> 3. **Baixar** os PDFs de inteiro teor;\n",
    "> 4. Construir um **corpus** a partir dos pdfs para análises textuais.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c666330a",
   "metadata": {},
   "source": [
    "## 0) Preparação do ambiente\n",
    "\n",
    "Vamos começar carregando os pacotes que vamos utilizar. Se estiver rodando no seu ambiente local, vale instalar as dependências (apenas na primeira vez):\n",
    "\n",
    "```bash\n",
    "pip install requests pandas pdfminer.six PyPDF2 nltk unidecode\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98164966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports principais\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# leitura de PDFs\n",
    "from pdfminer.high_level import extract_text  # para extrair o texto\n",
    "from PyPDF2 import PdfReader                  # para contar páginas\n",
    "\n",
    "# NLP util\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from unidecode import unidecode\n",
    "\n",
    "# downloads de recursos do NLTK (somente se ainda não tiver)\n",
    "try:\n",
    "    _ = stopwords.words('portuguese')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    _ = stopwords.words('portuguese')\n",
    "\n",
    "# conferir versões rápidas\n",
    "pd.__version__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34b769c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1) API da Câmara dos Deputados — obtendo as proposições de PLs de 2025\n",
    "\n",
    "Nosso primeiro objetivo é entender como funciona uma API. Antes de avançar, visite a página da [API de Dados Abertos da Câmara dos Deputados](https://dadosabertos.camara.leg.br/swagger/api.html).\n",
    "\n",
    "Para obter os dados das proposições de PLs de 2025, vamos utilizar o endpoint `/proposicoes` com os parâmetros de sigla do tipo de proposição, ano e mais alguns parâmetros obrigatórios. Vamos limitar a 100 itens.\n",
    "\n",
    "A API da Câmara é bem simples de usar em Python: vamos construir uma requisição HTTP com `requests.get`, passar os parâmetros em `params` e, no retorno JSON, acessar o campo `dados` com as informações básicas das proposições.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4467e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = \"https://dadosabertos.camara.leg.br/api/v2/proposicoes\"\n",
    "\n",
    "params = {\n",
    "    \"siglaTipo\": \"PL\",\n",
    "    \"ano\": 2025,\n",
    "    \"ordem\": \"ASC\",\n",
    "    \"ordenarPor\": \"id\",\n",
    "    \"itens\": 100\n",
    "}\n",
    "\n",
    "resp = requests.get(BASE, headers={\"Accept\": \"application/json\"}, params=params)\n",
    "resp.raise_for_status()\n",
    "dados = resp.json().get(\"dados\", [])\n",
    "df_proposicoes_simples = pd.DataFrame(dados)\n",
    "df_proposicoes_simples.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63961bbf",
   "metadata": {},
   "source": [
    "> **Nota rápida sobre paginação da API**: se você quiser **mais que 100** registros, inclua o parâmetro `pagina` em loop e agregue os resultados.  \n",
    "\n",
    "O resultado do processo é um `DataFrame` que contém uma informação essencial para o próximo passo, que é o **id** da proposição. Vamos agora utilizar estes ids em outro endpoint da API para obter informações detalhadas das proposições.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131cca34",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2) Função para buscar o **detalhe** de uma proposição\n",
    "\n",
    "Agora, para cada `id` da etapa anterior, buscamos o detalhe no endpoint `/proposicoes/{id}`. A função abaixo retorna um dicionário “achatado” com campos úteis, incluindo o **`urlInteiroTeor`**, que é o endereço que contém o documento da proposição.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f01742",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, Any\n",
    "\n",
    "def obter_proposicao_detalhe(id: int) -> Optional[Dict[str, Any]]:\n",
    "    url = f\"https://dadosabertos.camara.leg.br/api/v2/proposicoes/{id}\"\n",
    "    r = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
    "    if not r.ok:\n",
    "        return None\n",
    "    proposicao = r.json().get(\"dados\", {})\n",
    "\n",
    "    status_proposicao = proposicao.get(\"statusProposicao\") or {}\n",
    "\n",
    "    return {\n",
    "        \"id\": proposicao.get(\"id\"),\n",
    "        \"siglaTipo\": proposicao.get(\"siglaTipo\"),\n",
    "        \"numero\": proposicao.get(\"numero\"),\n",
    "        \"ano\": proposicao.get(\"ano\"),\n",
    "        \"descricaoTipo\": proposicao.get(\"descricaoTipo\"),\n",
    "        \"ementa\": proposicao.get(\"ementa\"),\n",
    "        \"ementaDetalhada\": proposicao.get(\"ementaDetalhada\"),\n",
    "        \"keywords\": proposicao.get(\"keywords\"),\n",
    "        \"dataApresentacao\": proposicao.get(\"dataApresentacao\"),\n",
    "        \"urlInteiroTeor\": proposicao.get(\"urlInteiroTeor\"),\n",
    "        \"uriAutores\": proposicao.get(\"uriAutores\"),\n",
    "        # status\n",
    "        \"status_dataHora\": status_proposicao.get(\"dataHora\"),\n",
    "        \"status_sequencia\": status_proposicao.get(\"sequencia\"),\n",
    "        \"status_siglaOrgao\": status_proposicao.get(\"siglaOrgao\"),\n",
    "        \"status_regime\": status_proposicao.get(\"regime\"),\n",
    "        \"status_descricaoTramitacao\": status_proposicao.get(\"descricaoTramitacao\"),\n",
    "        \"status_codTipoTramitacao\": status_proposicao.get(\"codTipoTramitacao\"),\n",
    "        \"status_descricaoSituacao\": status_proposicao.get(\"descricaoSituacao\"),\n",
    "        \"status_codSituacao\": status_proposicao.get(\"codSituacao\"),\n",
    "        \"status_despacho\": status_proposicao.get(\"despacho\"),\n",
    "        \"status_ambito\": status_proposicao.get(\"ambito\"),\n",
    "        \"status_apreciacao\": status_proposicao.get(\"apreciacao\"),\n",
    "    }\n",
    "\n",
    "# versão \"safe\": não quebra o loop se algum id falhar\n",
    "def safe_obter_proposicao_detalhe(id: int) -> Optional[Dict[str, Any]]:\n",
    "    try:\n",
    "        return obter_proposicao_detalhe(id)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# aplica sobre todos os ids (com um pequeno intervalo para não sobrecarregar a API)\n",
    "detalhes = []\n",
    "for x in df_proposicoes_simples[\"id\"].tolist():\n",
    "    time.sleep(0.1)\n",
    "    d = safe_obter_proposicao_detalhe(int(x))\n",
    "    if d is not None:\n",
    "        detalhes.append(d)\n",
    "\n",
    "df_proposicoes = pd.DataFrame(detalhes)\n",
    "df_proposicoes.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca9b86b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3) **Baixar** os PDFs de inteiro teor\n",
    "\n",
    "Com os URLs dos documentos de inteiro teor, podemos fazer o download da coleção completa. Vamos criar uma função simples de download. Usamos `try/except` e uma versão “safe” para evitar quebra do código. Cada arquivo será salvo com o nome `\"id\" + \".pdf\"`, para podermos associar às proposições por id.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a19d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baixar_inteiro_teor(id: int, url: str, diretorio: str = \"inteiro_teor\"):\n",
    "    if not url:\n",
    "        return\n",
    "    Path(diretorio).mkdir(parents=True, exist_ok=True)\n",
    "    destino = Path(diretorio) / f\"{id}.pdf\"\n",
    "    try:\n",
    "        r = requests.get(url, stream=True, timeout=60)\n",
    "        r.raise_for_status()\n",
    "        with open(destino, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "    except Exception:\n",
    "        # falhou a request; seguimos em frente\n",
    "        pass\n",
    "\n",
    "def safe_baixar_inteiro_teor(id: int, url: str, diretorio: str = \"inteiro_teor\"):\n",
    "    try:\n",
    "        baixar_inteiro_teor(id, url, diretorio)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# realiza os downloads com pequeno intervalo\n",
    "outdir = \"inteiro_teor\"\n",
    "for _, row in df_proposicoes.iterrows():\n",
    "    time.sleep(0.1)\n",
    "    safe_baixar_inteiro_teor(int(row[\"id\"]), row.get(\"urlInteiroTeor\"), outdir)\n",
    "\n",
    "# lista de arquivos baixados (pode estar vazia se você não executou os downloads)\n",
    "arquivos = sorted([p.name for p in Path(outdir).glob(\"*.pdf\")])\n",
    "len(arquivos), arquivos[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91af7020",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4) Lendo os PDFs e montando um **corpus**\n",
    "\n",
    "Agora a parte final da coleta dos dados: transformar os PDFs baixados em uma única tabela com **id**, **número de páginas**, **texto completo** e **tamanho** (em caracteres). Para termos um texto único por proposição, vamos \"colar\" os textos com `\\n`.  \n",
    "**Pressuposto**: os documentos têm OCR (para 2025 isso é razoável). Caso não tenham caracteres reconhecíveis, será necessário aplicar OCR antes (ex.: Tesseract) e só depois extrair o texto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afcb383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ler_pdf(arquivo_pdf: str, pasta: str = \"inteiro_teor\"):\n",
    "    caminho = Path(pasta) / arquivo_pdf\n",
    "    texto = \"\"\n",
    "    try:\n",
    "        texto = extract_text(str(caminho)) or \"\"\n",
    "    except Exception:\n",
    "        texto = \"\"\n",
    "\n",
    "    # conta páginas com PyPDF2\n",
    "    n_paginas = None\n",
    "    try:\n",
    "        reader = PdfReader(str(caminho))\n",
    "        n_paginas = len(reader.pages)\n",
    "    except Exception:\n",
    "        n_paginas = None\n",
    "\n",
    "    _id = arquivo_pdf.replace(\".pdf\", \"\")\n",
    "    return {\n",
    "        \"id\": _id,\n",
    "        \"n_paginas\": n_paginas,\n",
    "        \"text\": texto,\n",
    "        \"n_caracteres\": len(texto),\n",
    "    }\n",
    "\n",
    "corpus_docs = pd.DataFrame([ler_pdf(a) for a in arquivos])\n",
    "corpus_docs.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13447b91",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5) **(Opcional)**: já dá pra brincar com análises de texto\n",
    "\n",
    "Abaixo vai um exemplo equivalente ao que faríamos com `tidytext`: tokenização simples, remoção de stopwords e contagem de palavras mais frequentes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da98987",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_pt = set(stopwords.words('portuguese'))\n",
    "\n",
    "def preprocess_text(s: str) -> str:\n",
    "    # padroniza: minúsculas, remove acentos\n",
    "    s = (s or \"\").lower()\n",
    "    s = unidecode(s)\n",
    "    return s\n",
    "\n",
    "# tokenização básica por split em espaços (para um pipeline mais robusto, considere regex/Spacy)\n",
    "tokens = []\n",
    "for _, row in corpus_docs.iterrows():\n",
    "    txt = preprocess_text(row.get(\"text\", \"\"))\n",
    "    words = [w for w in txt.split() if w.isalpha() and w not in stop_pt]\n",
    "    tokens.extend(words)\n",
    "\n",
    "top_palavras = (\n",
    "    pd.Series(tokens, name=\"word\")\n",
    "    .value_counts()\n",
    "    .head(30)\n",
    "    .rename_axis(\"word\")\n",
    "    .reset_index(name=\"n\")\n",
    ")\n",
    "top_palavras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d848cd",
   "metadata": {},
   "source": [
    "> Dica: se quiser padronizar antes (minúsculas, remover pontuação, tirar acentos), o bloco `preprocess_text` já resolve boa parte; para limpeza adicional, você pode aplicar regex para remover pontuação antes de tokenizar. Isso costuma deixar o corpus mais fácil de trabalhar.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
