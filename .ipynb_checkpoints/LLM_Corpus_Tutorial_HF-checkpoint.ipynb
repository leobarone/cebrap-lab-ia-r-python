{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LLM Applications on a Legal Corpus (Python + Hugging Face)\n",
        "\n",
        "This notebook demonstrates end-to-end LLM-based applications for a corpus of Brazilian legislative texts (PLs). It assumes a dataframe named `corpus_docs` with columns `id` and `text`.\n",
        "\n",
        "**Modules shown:**\n",
        "1. Setup & data load\n",
        "2. Embeddings & Semantic Search (Sentence-Transformers)\n",
        "3. Zero-shot Classification (XNLI)\n",
        "4. Named Entity Recognition (NER)\n",
        "5. Summarization (mT5)\n",
        "6. Extractive QA (span-based)\n",
        "7. RAG: Retrieval-Augmented Generation (Retriever + Generator)\n",
        "8. Controlled Generation (bulleted summaries)\n",
        "9. Anonymization via NER\n",
        "\n",
        "---\n",
        "### Intuition vs. Technique (Quick Recap)\n",
        "- **Embeddings**: map texts/queries to a vector space; nearest neighbors = semantic similarity.\n",
        "- **Zero-shot**: cast labels as natural-language hypotheses and compute entailment.\n",
        "- **NER**: token classification to tag entities; aggregate subword spans.\n",
        "- **Summarization**: encoder-decoder seq2seq; hierarchical for long docs.\n",
        "- **Extractive QA**: predict start/end indices of an answer span in a context.\n",
        "- **RAG**: retrieve top-k chunks → condition a generator on them → answer with citations.\n",
        "- **Controlled Generation**: instruction-tuned decoding constrained by prompts.\n",
        "- **Anonymization**: detect entities and mask them; combine with regex rules when needed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) Setup\n",
        "Install dependencies. If running on Colab/Kaggle, you can use `pip`. On local environments, consider creating a venv/conda env first.\n",
        "\n",
        "⚠️ **Note**: Restart the kernel after installation if packages were just installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install -U transformers==4.44.2 sentence-transformers==3.0.1 torch accelerate\n",
        "# !pip install -U faiss-cpu evaluate numpy pandas umap-learn hdbscan\n",
        "# Optional for PDF parsing: pdfminer.six or pypdf\n",
        "# !pip install -U pdfminer.six"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Imports & Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List, Dict, Any\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import textwrap\n",
        "\n",
        "def chunk_text(text: str, chunk_size: int = 1200) -> List[str]:\n",
        "    \"\"\"Split text into roughly chunk_size-character segments, by sentences when possible.\"\"\"\n",
        "    if text is None:\n",
        "        return []\n",
        "    text = str(text)\n",
        "    if len(text) <= chunk_size:\n",
        "        return [text]\n",
        "    # naive sentence split\n",
        "    sents = text.replace('\\r', ' ').split('. ')\n",
        "    out, cur = [], ''\n",
        "    for s in sents:\n",
        "        s2 = s if s.endswith('.') else s + '.'\n",
        "        if len(cur) + len(s2) + 1 <= chunk_size:\n",
        "            cur = (cur + ' ' + s2).strip()\n",
        "        else:\n",
        "            if cur:\n",
        "                out.append(cur)\n",
        "            cur = s2\n",
        "    if cur:\n",
        "        out.append(cur)\n",
        "    return out\n",
        "\n",
        "def cosine_sim_matrix(A: np.ndarray, B: np.ndarray) -> np.ndarray:\n",
        "    A = A / (np.linalg.norm(A, axis=1, keepdims=True) + 1e-9)\n",
        "    B = B / (np.linalg.norm(B, axis=1, keepdims=True) + 1e-9)\n",
        "    return A @ B.T\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Load your corpus\n",
        "Expecting a CSV with columns: `id,text`. If you already have a dataframe, skip this and set `corpus_docs` directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: load from CSV (one id per row)\n",
        "# corpus_docs = pd.read_csv(\"corpus_docs.csv\")\n",
        "# For demo purposes, build a tiny placeholder:\n",
        "corpus_docs = pd.DataFrame({\n",
        "    'id': ['PL1','PL2'],\n",
        "    'text': [\n",
        "        'Altera o Código Penal para estabelecer novas regras... Justificativa: ...',\n",
        "        'Dispõe sobre política pública de saúde... Art. 1º ... prazo de 30 dias ...'\n",
        "    ]\n",
        "})\n",
        "len(corpus_docs), corpus_docs.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Embeddings & Semantic Search (Bi-Encoder)\n",
        "**Intuition.** Map documents and queries into the same vector space; nearest neighbors ≈ semantically closest.\n",
        "\n",
        "**Technique.** Sentence-Transformers bi-encoder trained with contrastive/NLI objectives; cosine (or dot) similarity; scalable via ANN.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "st_model_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "embedder = SentenceTransformer(st_model_name)\n",
        "\n",
        "docs = corpus_docs['text'].fillna('').astype(str).tolist()\n",
        "doc_emb = embedder.encode(docs, normalize_embeddings=True)\n",
        "doc_emb = np.asarray(doc_emb)\n",
        "\n",
        "def semantic_search(query: str, k: int = 5) -> pd.DataFrame:\n",
        "    qv = embedder.encode([query], normalize_embeddings=True)\n",
        "    S = cosine_sim_matrix(np.asarray(qv), doc_emb)[0]\n",
        "    idx = np.argsort(-S)[:k]\n",
        "    return pd.DataFrame({\n",
        "        'rank': np.arange(1, len(idx)+1),\n",
        "        'id': corpus_docs.loc[idx, 'id'].values,\n",
        "        'score': S[idx],\n",
        "        'snippet': [textwrap.shorten(corpus_docs.loc[i,'text'], width=180) for i in idx]\n",
        "    })\n",
        "\n",
        "semantic_search(\"prisões cautelares\", k=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Zero-shot Classification (XNLI)\n",
        "**Intuition.** Turn labels into hypotheses (natural language), then measure entailment probability.\n",
        "\n",
        "**Technique.** NLI head estimates P(entailment | premise=text, hypothesis=label). Multi-label via threshold per label.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "zs = pipeline(\"zero-shot-classification\", model=\"joeddav/xlm-roberta-large-xnli\")\n",
        "labels = [\"segurança pública\",\"tributação\",\"direitos humanos\",\"direito penal\",\n",
        "          \"processo civil\",\"política social\",\"administração pública\"]\n",
        "\n",
        "def classify_zero_shot(text: str, candidate_labels=labels) -> pd.DataFrame:\n",
        "    out = zs(text, candidate_labels=candidate_labels, multi_label=True)\n",
        "    return pd.DataFrame({\"label\": out[\"labels\"], \"score\": out[\"scores\"]}).sort_values(\"score\", ascending=False)\n",
        "\n",
        "classify_zero_shot(corpus_docs.loc[0,'text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Named Entity Recognition (NER)\n",
        "**Intuition.** Tag names of people, organizations, and locations to enrich metadata or anonymize.\n",
        "\n",
        "**Technique.** Token classification; aggregate subword spans into entity spans."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ner = pipeline(\"token-classification\", model=\"Davlan/xlm-roberta-base-ner-hrl\", aggregation_strategy=\"simple\")\n",
        "\n",
        "def ner_extract(text: str) -> pd.DataFrame:\n",
        "    ents = ner(text)\n",
        "    if not ents:\n",
        "        return pd.DataFrame(columns=[\"entity\",\"word\",\"score\",\"start\",\"end\"])    \n",
        "    return pd.DataFrame([{ 'entity':e.get('entity_group', e.get('entity')), 'word':e['word'], 'score':e['score'], 'start':e['start'], 'end':e['end']} for e in ents])\n",
        "\n",
        "ner_extract(corpus_docs.loc[0,'text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Summarization (mT5)\n",
        "**Intuition.** Ask a seq2seq model to compress the core content of each document.\n",
        "\n",
        "**Technique.** mT5 (multilingual) trained on news-like summarization. For long texts, do hierarchical: chunk → local summaries → meta-summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "summarizer = pipeline(\"summarization\", model=\"csebuetnlp/mT5_multilingual_XLSum\")\n",
        "\n",
        "def summarize_doc(text: str, min_len: int=60, max_len: int=180, chunk_chars: int=2500) -> str:\n",
        "    parts = []\n",
        "    for ch in chunk_text(text, chunk_chars):\n",
        "        if not ch.strip():\n",
        "            continue\n",
        "        out = summarizer(ch, min_length=min_len, max_length=max_len)\n",
        "        parts.append(out[0]['summary_text'])\n",
        "    return \"\\n\".join(parts)\n",
        "\n",
        "print(summarize_doc(corpus_docs.loc[0,'text']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Extractive QA (span-based)\n",
        "**Intuition.** For factoid questions, predict the exact answer span from a given context.\n",
        "\n",
        "**Technique.** MRC with start/end indices; you must select an appropriate context chunk first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "qa_extractive = pipeline(\"question-answering\", model=\"deepset/xlm-roberta-base-squad2\")\n",
        "\n",
        "# choose a simple context (for demo); in practice select best chunk via semantic search\n",
        "context = corpus_docs.loc[1,'text']\n",
        "qa_extractive(question=\"Qual é o prazo mencionado?\", context=context)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) RAG: Retrieval-Augmented Generation\n",
        "**Intuition.** Retrieve the most relevant chunks, then generate an answer strictly grounded in that context (with citations).\n",
        "\n",
        "**Technique.** Bi-encoder retrieval (Sentence-Transformers) + instruction-tuned generator (e.g., FLAN-T5)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build chunk-level index\n",
        "def make_chunk_df(df: pd.DataFrame, chunk_chars: int=900) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    for _, row in df.iterrows():\n",
        "        chunks = chunk_text(row['text'], chunk_chars)\n",
        "        for i, ch in enumerate(chunks, start=1):\n",
        "            if ch.strip():\n",
        "                rows.append({'id': row['id'], 'chunk_id': i, 'text': ch})\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "corpus_chunks = make_chunk_df(corpus_docs, chunk_chars=900)\n",
        "chunk_emb = embedder.encode(corpus_chunks['text'].tolist(), normalize_embeddings=True)\n",
        "chunk_emb = np.asarray(chunk_emb)\n",
        "\n",
        "def retrieve(query: str, k: int=4) -> pd.DataFrame:\n",
        "    qv = embedder.encode([query], normalize_embeddings=True)\n",
        "    S = cosine_sim_matrix(np.asarray(qv), chunk_emb)[0]\n",
        "    idx = np.argsort(-S)[:k]\n",
        "    return pd.DataFrame({\n",
        "        'rank': np.arange(1, len(idx)+1),\n",
        "        'id': corpus_chunks.loc[idx, 'id'].values,\n",
        "        'chunk_id': corpus_chunks.loc[idx, 'chunk_id'].values,\n",
        "        'score': S[idx],\n",
        "        'text': corpus_chunks.loc[idx, 'text'].values\n",
        "    })\n",
        "\n",
        "generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
        "\n",
        "def answer_with_context(question: str, k: int=4, max_new_tokens: int=220) -> Dict[str, Any]:\n",
        "    ctx = retrieve(question, k)\n",
        "    prompt = (\n",
        "        \"Responda em português, usando APENAS o CONTEXTO a seguir. Cite trechos entre aspas.\\n\" +\n",
        "        f\"Pergunta: {question}\\n\\nContexto:\\n\" +\n",
        "        \"\\n\".join([\"- \" + textwrap.shorten(t, width=500) for t in ctx['text'].tolist()]) +\n",
        "        \"\\n\\nResposta:\" )\n",
        "    out = generator(prompt, max_new_tokens=max_new_tokens)\n",
        "    return {\"answer\": out[0]['generated_text'], \"retrieved\": ctx}\n",
        "\n",
        "qa = answer_with_context(\"Quais dispositivos são alterados?\", k=3)\n",
        "qa['answer']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Controlled Generation (bulleted summaries)\n",
        "**Intuition.** Instruct a model to produce concise, structured bullet points in plain language.\n",
        "\n",
        "**Technique.** Instruction-tuned generation with decoding constraints (max tokens, repetition penalty if needed)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def bullet_summary(text: str, bullets: int=5) -> str:\n",
        "    prompt = (\n",
        "        f\"Explique o seguinte Projeto de Lei em português simples, em {bullets} tópicos curtos e objetivos.\\n\\n\" +\n",
        "        textwrap.shorten(text, width=4000) +\n",
        "        \"\\n\\nTópicos:\" )\n",
        "    out = generator(prompt, max_new_tokens=bullets*60)\n",
        "    return out[0]['generated_text']\n",
        "\n",
        "print(bullet_summary(corpus_docs.loc[0,'text'], bullets=5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) Anonymization via NER\n",
        "**Intuition.** Detect entities and mask them for privacy while preserving analytical utility.\n",
        "\n",
        "**Technique.** Run NER and replace spans from end to start to avoid offset drift."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mask_pii(text: str) -> str:\n",
        "    ents = ner(text)\n",
        "    if not ents:\n",
        "        return text\n",
        "    out = text\n",
        "    # reverse by start index to avoid shifting\n",
        "    for e in sorted(ents, key=lambda x: x['start'], reverse=True):\n",
        "        label = e.get('entity_group', e.get('entity'))\n",
        "        if label in {\"PER\",\"ORG\",\"LOC\"}:  # adjust as needed\n",
        "            s, t = int(e['start']), int(e['end'])\n",
        "            out = out[:s] + '█'*(t-s) + out[t:]\n",
        "    return out\n",
        "\n",
        "mask_pii(\"João da Silva, do Ministério da Justiça, apresentou proposta em Brasília.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### Notes & Tips\n",
        "- For Portuguese summarization, `csebuetnlp/mT5_multilingual_XLSum` tends to produce more natural PT than BART-CNN.\n",
        "- For better RAG faithfulness, add a cross-encoder re-ranker to re-score the top-k.\n",
        "- Tune chunk sizes (900–1500 chars) and overlaps (10–20%).\n",
        "- Zero-shot is sensitive to label wordings; treat verbalizers and thresholds as hyperparameters.\n",
        "- Consider quantization (int8/int4) for larger generators (Llama/Mistral) if you have GPU constraints.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}